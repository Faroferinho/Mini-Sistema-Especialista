import math

import spacy
from collections import Counter

nlp = spacy.load("en_core_web_sm")


# This will get the tokens form a text
def get_tokens_from_text(text):
    return nlp.tokenizer(text)


# This will get the frequency of which individual words apear in the text
def get_word_frequency(tokens):
    return Counter(tokens)


# Remove the Punctuation from a sample text
def filter_punctuation(text):
    words = []
    for token in text:
        if token.is_alpha and not token.is_space:
            words.append(token)

    return words


# This function will do the perplexity calculation based on the function:
# PP(W) = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i)}
def get_perplexity(text):
    document = nlp(text)

    words = filter_punctuation(document)

    word_counts = get_word_frequency(words)

    total_words = len(words)
    unique_words = len(word_counts)

    probabilities = {
        word: count / total_words for word, count in word_counts.items()
    }

    log_sum = 0.0
    epsilon = 1e-10 / total_words

    for word in words:
        p_word = probabilities.get(word, epsilon)
        log_sum += math.log2(p_word)

    perplexity = 2 ** (- (1 / total_words) * log_sum)

    return perplexity


# Calculate
def calculate_burstiness(tokens):
    return False


example = "Text that was not generated by a Large Language Model"

tokensFromText = get_tokens_from_text(example)

frequency = get_word_frequency(tokensFromText)

example_perplexity = get_perplexity(example)

print(f"Example being Used: {example}")
print(f"Tokenized Text: {tokensFromText}")
print(f"Words Frequency: {frequency}")
print(f"Perplexity: {example_perplexity}")
